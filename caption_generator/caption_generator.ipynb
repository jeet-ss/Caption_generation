{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dee9099c8e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file in memory\n",
    "def load_file(file_name):\n",
    "    # open the file as read only\n",
    "    file = open(file_name, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#loading a pre-defined image features(if required)\n",
    "def load_data(filename):\n",
    "    file = load_file(filename)\n",
    "    dataset = list()\n",
    "    for line in file.split('\\n'):\n",
    "        #discard small ones\n",
    "        if len(line)>2:\n",
    "            continue\n",
    "        id = line.split('.')[0]\n",
    "        dataset.append(id)\n",
    "    return dataset\n",
    "\n",
    "#remove punctuations from captions\n",
    "def clean_captions(caption):\n",
    "    #tabe for removing punctuation, this make all punctuations as None\n",
    "    table = str('', '', string.punctuation)\n",
    "    for key, caps in caption.items():\n",
    "        #tokenize the data\n",
    "        caps = caps.split()\n",
    "        #lower case\n",
    "        caps = [word.lower() for word in caps]\n",
    "        #remove punctuation\n",
    "        caps = [word.translate(table) for word in caps]\n",
    "        #removing any stray letters as errands\n",
    "        caps = [word for word in caps if len(word)>1]\n",
    "        #save\n",
    "        captions[key] = ' '.join(caps)\n",
    "    return captions\n",
    "\n",
    "#load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    #load features which are part of our dataset\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "#test_train split\n",
    "def train_test_split(dataset):\n",
    "    sort = sorted(dataset)\n",
    "    return set(sort[:500]), set(sort[500:800])\n",
    "\n",
    "#fit tokenizer\n",
    "def create_tokenizer(captions):\n",
    "    lines = list(captions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "#create time sequences\n",
    "def create_seq(tokenizer, caps, image, max_length):\n",
    "    Ximages, Xseq, y = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    #encoding captions to integers\n",
    "    seq_num = tokenizer.texts_to_sequences([caps])[0]\n",
    "    #splitting sentences for input\n",
    "    for i in range(1, len(seq_num)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        #padding to max length\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        #one-hot encoding\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        #save\n",
    "        Ximages.append(image)\n",
    "        XSeq.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    return [Ximages, Xseq, y]\n",
    "\n",
    "#word2vec embedding\n",
    "def word2vec_embedding(tokenizer, vocab_size, max_length):\n",
    "    #load\n",
    "    embedding = load(open('word2vec_embedding.pkl', 'rb'))\n",
    "    dim = 100\n",
    "    trainable = False\n",
    "    #create a weight matrix for words in training captions\n",
    "    weights = zeros((vocab_size, dim))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word not in embedding:\n",
    "            continue\n",
    "        weigths[i] = embedding[word]\n",
    "    layer = Embedding(vocab_size, dimensions, weights=[weights], input_length=max_length, trainable=trainable, mask_zero=True)\n",
    "    return layer\n",
    "        \n",
    "#defining the working model\n",
    "def define_model(vocab_size, max_length):\n",
    "    #feature extractor\n",
    "    input1 = Input(shape(7,7,512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    #embedding\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    emb2 = word2vec_embedding(tokenizer, vocab_size, max_length)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    #merge\n",
    "    merge = concatenate([fe3, emb4])\n",
    "    #decoder part\n",
    "    lm2 = LSTM(256)(merge)\n",
    "    lm3 = Dense(500, activation = 'relu')(lm2)\n",
    "    out = Dense(vocab_size, activation = 'softmax')(lm3)\n",
    "    #summary\n",
    "    model = Model(inputs=[input1, input2], outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model\n",
    "\n",
    "#data generator for the model\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    #loop to train\n",
    "    while 1 :\n",
    "        keys = list(captions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximg, Xseq, y = list(), list(), list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                img_id = keys[j]\n",
    "                #get feature of image\n",
    "                image = features[image_id][0]\n",
    "                #captions\n",
    "                caps = captions[image_id]\n",
    "                #generate in-out \n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer,caps, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    Xseq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "                #yeild the data to the model\n",
    "                yeild [[array(Ximages), array(Xseq), array(y)]]\n",
    "\n",
    "#map integer to word\n",
    "def int_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer :\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "#generate image description\n",
    "def generate_caps(model, tokenizer, image, max_length):\n",
    "    #first input word\n",
    "    in_word = '<start>'\n",
    "    #prediction\n",
    "    for i in range(max_length):\n",
    "        #encode input string to integer\n",
    "        seq = tokenizer.texts_to_sequence([in_word])[0]\n",
    "        #pad input string to match the length\n",
    "        pad_seq = pad_sequences([seq], maxlen = max_length)\n",
    "        #predict next word\n",
    "        predict = model.predict([image, pad_seq], verbose=0)\n",
    "        #get corresponding integer\n",
    "        y_hat = argmax(predict)\n",
    "        #map integer to word\n",
    "        word = int_to_word(y_hat, tokenizer)\n",
    "        #fail-safe\n",
    "        if word is None:\n",
    "            break\n",
    "        in_word += ' ' + word\n",
    "        \n",
    "        if word == '<end>':\n",
    "            break\n",
    "    return in_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-59f1b98bb6ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load saved image data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#test-train split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fb9adf9eeadb>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#loading a pre-defined image features(if required)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fb9adf9eeadb>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# open the file as read only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# read all text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ' '"
     ]
    }
   ],
   "source": [
    "#load saved image data\n",
    "filename = ' '\n",
    "dataset = load_data(filename)\n",
    "#test-train split\n",
    "train , test = train_test_split(dataset)\n",
    "#load previously saved captions\n",
    "train_captions = clean_captions('captions.txt', train)\n",
    "test_features= clean_captions('captions.txt', test)\n",
    "#load saved image features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# determine the maximum sequence length\n",
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \n",
    "verbose = 2\n",
    "n_epochs = 50\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "#run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    #define model\n",
    "    model = define.model(vocab_size, max_length)\n",
    "    #fit model\n",
    "    model.fit_generator(data_generator(train_captions, train_features,max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_captions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_captions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
